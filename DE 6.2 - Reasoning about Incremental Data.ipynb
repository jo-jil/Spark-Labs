{"cells":[{"cell_type":"markdown","source":["<i18n value=\"988e4844-2372-4e5c-b603-bf59f9c0c638\"/>\n\n\n# Reasoning about Incremental Data\n\nSpark Structured Streaming extends the functionality of Apache Spark to allow for simplified configuration and bookkeeping when processing incremental datasets. In the past, much of the emphasis for streaming with big data has focused on reducing latency to provide near real time analytic insights. While Structured Streaming provides exceptional performance in achieving these goals, this lesson will focus more on the applications of incremental data processing.\n\nWhile incremental processing is not absolutely necessary to work successfully in the data lakehouse, our experience helping some of the world's largest companies derive insights from the world's largest datasets has led to the conclusion that many workloads can benefit substantially from an incremental processing approach. Many of the core features at the heart of Databricks have been optimized specifically to handle these ever-growing datasets.\n\nConsider the following datasets and use cases:\n* Data scientists need secure, de-identified, versioned access to frequently updated records in an operational database\n* Credit card transactions need to be compared to past customer behavior to identify and flag fraud\n* A multi-national retailer seeks to serve custom product recommendations using purchase history\n* Log files from distributed systems need to be analayzed to detect and respond to instabilities\n* Clickstream data from millions of online shoppers needs to be leveraged for A/B testing of UX\n\nThe above are just a small sample of datasets that grow incrementally and infinitely over time.\n\nIn this lesson, we'll explore the basics of working with Spark Structured Streaming to allow incremental processing of data. In the next lesson, we'll talk more about how this incremental processing model simplifies data processing in the data lakehouse.\n\n## Learning Objectives\nBy the end of this lesson, you should be able to:\n* Describe the programming model used by Spark Structured Streaming\n* Configure required options to perform a streaming read on a source\n* Describe the requirements for end-to-end fault tolerance\n* Configure required options to perform a streaming write to a sink"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9a4e2432-8271-4151-91b6-7b72b75f931e","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"203d2cf2-e366-4b68-8e45-e17186a9503b\"/>\n\n\n\n## Getting Started\n\nRun the following cell to configure our \"classroom.\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dc70ba8a-619a-4bb4-8357-a77b748c26bc","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup-06.2"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2830708a-5e1f-4f44-b69a-649ed6293505","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Python interpreter will be restarted.\nPython interpreter will be restarted.\n"]},{"output_type":"stream","output_type":"stream","name":"stdout","text":["\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/data-engineering-with-databricks/v02\"\n\nValidating the locally installed datasets:\n| listing local files...(10 seconds)\n| completed (10 seconds total)\n\nCreating & using the schema \"jtschopp_k017_dbacademy_dewd\"...(1 seconds)\n\nLoading the file 01.json to the dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks/tracker/01.json\nPredefined tables in \"jtschopp_k017_dbacademy_dewd\":\n| bronze\n\nPredefined paths variables:\n| DA.paths.working_dir: dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks\n| DA.paths.user_db:     dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks/database.db\n| DA.paths.datasets:    dbfs:/mnt/dbacademy-datasets/data-engineering-with-databricks/v02\n| DA.paths.checkpoints: dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks/_checkpoints\n\nSetup completed (58 seconds)\n"]}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"bb701ed4-32db-4e97-bd72-0b90acaeca16\"/>\n\n\n\n## Treating Infinite Data as a Table\n\nThe magic behind Spark Structured Streaming is that it allows users to interact with ever-growing data sources as if they were just a static table of records.\n\n<img src=\"http://spark.apache.org/docs/latest/img/structured-streaming-stream-as-a-table.png\" width=\"800\"/>\n\nIn the graphic above, a **data stream** describes any data source that grows over time. New data in a data stream might correspond to:\n* A new JSON log file landing in cloud storage\n* Updates to a database captured in a CDC feed\n* Events queued in a pub/sub messaging feed\n* A CSV file of sales closed the previous day\n\nMany organizations have traditionally taken an approach of reprocessing the entire source dataset each time they want to update their results. Another approach would be to write custom logic to only capture those files or records that have been added since the last time an update was run.\n\nStructured Streaming lets us define a query against the data source and automatically detect new records and propagate them through previously defined logic. \n\n**Spark Structured Streaming is optimized on Databricks to integrate closely with Delta Lake and Auto Loader.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3d7a693b-9e3c-43f0-900e-e0368c14b806","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"08b87ec8-7f2b-4638-a93f-c6cf132ab1f0\"/>\n\n\n## Basic Concepts\n\n- The developer defines an **input table** by configuring a streaming read against a **source**. The syntax for doing this is similar to working with static data.\n- A **query** is defined against the input table. Both the DataFrames API and Spark SQL can be used to easily define transformations and actions against the input table.\n- This logical query on the input table generates the **results table**. The results table contains the incremental state information of the stream.\n- The **output** of a streaming pipeline will persist updates to the results table by writing to an external **sink**. Generally, a sink will be a durable system such as files or a pub/sub messaging bus.\n- New rows are appended to the input table for each **trigger interval**. These new rows are essentially analogous to micro-batch transactions and will be automatically propagated through the results table to the sink.\n\n<img src=\"http://spark.apache.org/docs/latest/img/structured-streaming-model.png\" width=\"800\"/>\n\n\nFor more information, see the analogous section in the <a href=\"http://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#basic-concepts\" target=\"_blank\">Structured Streaming Programming Guide</a> (from which several images have been borrowed)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9eb11ccd-538b-455a-aced-82ad2145d721","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"c0f129fa-daf9-4d01-8454-fa040442457f\"/>\n\n\n## End-to-end Fault Tolerance\n\nStructured Streaming ensures end-to-end exactly-once fault-tolerance guarantees through _checkpointing_ (discussed below) and <a href=\"https://en.wikipedia.org/wiki/Write-ahead_logging\" target=\"_blank\">Write Ahead Logs</a>.\n\nStructured Streaming sources, sinks, and the underlying execution engine work together to track the progress of stream processing. If a failure occurs, the streaming engine attempts to restart and/or reprocess the data.\nFor best practices on recovering from a failed streaming query see <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/production.html#recover-from-query-failures\" target=\"_blank\">docs</a>.\n\nThis approach _only_ works if the streaming source is replayable; replayable sources include cloud-based object storage and pub/sub messaging services.\n\nAt a high level, the underlying streaming mechanism relies on a couple of approaches:\n\n* First, Structured Streaming uses checkpointing and write-ahead logs to record the offset range of data being processed during each trigger interval.\n* Next, the streaming sinks are designed to be _idempotent_ - that is, multiple writes of the same data (as identified by the offset) do _not_ result in duplicates being written to the sink.\n\nTaken together, replayable data sources and idempotent sinks allow Structured Streaming to ensure **end-to-end, exactly-once semantics** under any failure condition."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"55e0c7e7-0a7d-4153-bc4a-c3d5af9fa3e0","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"57f9dadd-71ff-41cd-bf26-a3b968e68fba\"/>\n\n\n\n## Reading a Stream\n\nThe **`spark.readStream()`** method returns a **`DataStreamReader`** used to configure and query the stream.\n\nIn the previous lesson, we saw code configured for incrementally reading with Auto Loader. Here, we'll show how easy it is to incrementally read a Delta Lake table.\n\nThe code uses the PySpark API to incrementally read a Delta Lake table named **`bronze`** and register a streaming temp view named **`streaming_tmp_vw`**.\n\n**NOTE**: A number of optional configurations (not shown here) can be set when configuring incremental reads, the most important of which allows you to <a href=\"https://docs.databricks.com/delta/delta-streaming.html#limit-input-rate\" target=\"_blank\">limit the input rate</a>."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3a0ad453-3c3f-4058-bacf-4388a4059671","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["(spark.readStream\n    .table(\"bronze\")\n    .createOrReplaceTempView(\"streaming_tmp_vw\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6f5fede2-26d2-4089-88eb-67e775b5dcc3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"4fadee61-b02e-4601-a3b8-26c2c9f06160\"/>\n\n\n\nWhen we execute a query on a streaming temporary view, we'll continue to update the results of the query as new data arrives in the source.\n\nThink of a query executed against a streaming temp view as an **always-on incremental query**.\n\n**NOTE**: Generally speaking, unless a human is actively monitoring the output of a query during development or live dashboarding, we won't return streaming results to a notebook."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6d43d7aa-35c7-428f-8743-9ee875dfce0c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nSELECT * FROM streaming_tmp_vw"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"b802e3a4-f8c8-46b5-8057-376e0723c43b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"147067e0-bb54-4556-849f-00b9710e7c48\"/>\n\n\nYou will recognize the data as being the same as the Delta table written out in our previous lesson.\n\nBefore continuing, click **`Stop Execution`** at the top of the notebook, **`Cancel`** immediately under the cell, or run the following cell to stop all active streaming queries."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"30cdad24-35f5-417c-ba0c-b898f510ae43","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["for s in spark.streams.active:\n    print(\"Stopping \" + s.id)\n    s.stop()\n    s.awaitTermination()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7f31a9a9-821a-4f0d-b0bb-a84a9c725851","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Stopping 5a8018e0-7e91-4709-8cf1-a36d8c62d41e\n"]}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"2daca6ba-fca5-40d9-ba5f-b97cf609b20e\"/>\n\n\n## Working with Streaming Data\nWe can execute most transformation against streaming temp views the same way we would with static data. Here, we'll run a simple aggregation to get counts of records for each **`device_id`**.\n\nBecause we are querying a streaming temp view, this becomes a streaming query that executes indefinitely, rather than completing after retrieving a single set of results. For streaming queries like this, Databricks Notebooks include interactive dashboards that allow users to monitor streaming performance. Explore this below.\n\nOne important note regarding this example: this is merely displaying an aggregation of input as seen by the stream. **None of these records are being persisted anywhere at this point.**"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"955b67d7-4b98-434c-845f-b26028a46665","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nSELECT device_id, count(device_id) AS total_recordings\nFROM streaming_tmp_vw\nGROUP BY device_id"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"0c1ff7be-42b4-4384-a8d0-bad042ee418f","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"99d062fc-28c0-4b88-955e-f7d6e80ee472\"/>\n\n\nBefore continuing, click **`Stop Execution`** at the top of the notebook, **`Cancel`** immediately under the cell, or run the following cell to stop all active streaming queries."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"47218051-eca7-4c21-9604-35f0ade8f066","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["for s in spark.streams.active:\n    print(\"Stopping \" + s.id)\n    s.stop()\n    s.awaitTermination()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c5ce4fba-1cd2-4b93-8903-abc856aeaa0b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Stopping b7153592-29fd-47a3-8bc8-b0bfdcd0d960\n"]}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"c257cb96-badc-473c-873a-0fb4ced7c825\"/>\n\n\n## Unsupported Operations\n\nMost operations on a streaming DataFrame are identical to a static DataFrame. There are <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#unsupported-operations\" target=\"_blank\">some exceptions to this</a>.\n\nConsider the model of the data as a constantly appending table. Sorting is one of a handful of operations that is either too complex or logically not possible to do when working with streaming data.\n\nA full discussion of these exceptions is out of scope for this course. Note that advanced streaming methods like windowing and watermarking can be used to add additional functionality to incremental workloads.\n\nUncomment and run the following cell how this failure may appear:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e1348f01-6a9c-430f-b21c-a2c566475261","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# %sql\n# SELECT * \n# FROM streaming_tmp_vw\n# ORDER BY time"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a6f18731-3c24-4071-a10b-45ae84e3bf4c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"fbde5075-43da-4c12-882c-e6da63bdb026\"/>\n\n\n## Persisting Streaming Results\n\nIn order to persist incremental results, we need to pass our logic back to the PySpark Structured Streaming DataFrames API.\n\nAbove, we created a temp view from a PySpark streaming DataFrame. If we create another temp view from the results of a query against a streaming temp view, we'll again have a streaming temp view."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1faa23a9-6b5b-49ad-9b61-be9ade8be463","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMP VIEW device_counts_tmp_vw AS (\n  SELECT device_id, COUNT(device_id) AS total_recordings\n  FROM streaming_tmp_vw\n  GROUP BY device_id\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"9ba3f6c3-4134-4826-9ce6-32479fe4c7e2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"db473405-f8b5-4fc7-9f87-c669b3d8e7d8\"/>\n\n\n\n## Writing a Stream\n\nTo persist the results of a streaming query, we must write them out to durable storage. The **`DataFrame.writeStream`** method returns a **`DataStreamWriter`** used to configure the output.\n\nWhen writing to Delta Lake tables, we typically will only need to worry about 3 settings, discussed here.\n\n### Checkpointing\n\nDatabricks creates checkpoints by storing the current state of your streaming job to cloud storage.\n\nCheckpointing combines with write ahead logs to allow a terminated stream to be restarted and continue from where it left off.\n\nCheckpoints cannot be shared between separate streams. A checkpoint is required for every streaming write to ensure processing guarantees.\n\n### Output Modes\n\nStreaming jobs have output modes similar to static/batch workloads. <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html#output-modes\" target=\"_blank\">More details here</a>.\n\n| Mode   | Example | Notes |\n| ------------- | ----------- | --- |\n| **Append** | **`.outputMode(\"append\")`**     | **This is the default.** Only newly appended rows are incrementally appended to the target table with each batch |\n| **Complete** | **`.outputMode(\"complete\")`** | The Results Table is recalculated each time a write is triggered; the target table is overwritten with each batch |\n\n\n### Trigger Intervals\n\nWhen defining a streaming write, the **`trigger`** method specifies when the system should process the next set of data..\n\n\n| Trigger Type                           | Example | Behavior |\n|----------------------------------------|----------|----------|\n| Unspecified                 |  | **This is the default.** This is equivalent to using **`processingTime=\"500ms\"`** |\n| Fixed interval micro-batches      | **`.trigger(processingTime=\"2 minutes\")`** | The query will be executed in micro-batches and kicked off at the user-specified intervals |\n| Triggered micro-batch               | **`.trigger(once=True)`** | The query will execute a single micro-batch to process all the available data and then stop on its own |\n| Triggered micro-batches       | **`.trigger(availableNow=True)`** | The query will execute multiple micro-batches to process all the available data and then stop on its own |\n\nTriggers are specified when defining how data will be written to a sink and control the frequency of micro-batches. By default, Spark will automatically detect and process all data in the source that has been added since the last trigger.\n\n**NOTE:** **`Trigger.AvailableNow`**</a> is a new trigger type that is available in DBR 10.1 for Scala only and available in DBR 10.2 and above for Python and Scala."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9aba1bd1-a8de-438d-a58a-4414c657c0ef","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"a8df4901-05f0-41cd-944d-935c21fd40fc\"/>\n\n\n## Pulling It All Together\n\nThe code below demonstrates using **`spark.table()`** to load data from a streaming temp view back to a DataFrame. Note that Spark will always load streaming views as a streaming DataFrame and static views as static DataFrames (meaning that incremental processing must be defined with read logic to support incremental writing).\n\nIn this first query, we'll demonstrate using **`trigger(availableNow=True)`** to perform incremental batch processing."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7a8f6e67-1518-4cd6-a4a4-3474441caa3c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["(spark.table(\"device_counts_tmp_vw\")                               \n    .writeStream                                                \n    .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/silver\")\n    .outputMode(\"complete\")\n    .trigger(availableNow=True)\n    .table(\"device_counts\")\n    .awaitTermination() # This optional method blocks execution of the next cell until the incremental batch write has succeeded\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"342eb98f-4c4f-49e1-935f-fbe59999b891","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"0e13fc25-53ff-4c40-a93b-e3c3c5dc2644\"/>\n\n\nBelow, we change our trigger method to change this query from a triggered incremental batch to an always-on query triggered every 4 seconds.\n\n**NOTE**: As we start this query, no new records exist in our source table. We'll add new data shortly."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"54e363c4-348e-4478-84bb-a160c5c6ecf0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["query = (spark.table(\"device_counts_tmp_vw\")                               \n              .writeStream                                                \n              .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/silver\")\n              .outputMode(\"complete\")\n              .trigger(processingTime='4 seconds')\n              .table(\"device_counts\"))\n\n# Like before, wait until our stream has processed some data\nDA.block_until_stream_is_ready(query)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f65b5982-e7bd-4297-a0f7-37a57fb09e9f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Processed 0 of 2 batches...\nProcessed 1 of 2 batches...\nProcessed 1 of 2 batches...\nProcessed 1 of 2 batches...\nProcessed 2 of 2 batches...\nThe stream is now active with 2 batches having been processed.\n"]}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"d42a3739-71d7-4b9c-a1d2-a32feea1228f\"/>\n\n\n## Querying the Output\nNow let's query the output we've written from SQL. Because the result is a table, we only need to deserialize the data to return the results.\n\nBecause we are now querying a table (not a streaming DataFrame), the following will **not** be a streaming query."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"56fda226-5bf2-48a7-962c-538d70620c99","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nSELECT *\nFROM device_counts"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"2b5e4daa-975d-42fd-ab0e-ce2f70600dc7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["37",1977],["17",1475],["23",2460]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"device_id","type":"\"string\"","metadata":"{}"},{"name":"total_recordings","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>device_id</th><th>total_recordings</th></tr></thead><tbody><tr><td>37</td><td>1977</td></tr><tr><td>17</td><td>1475</td></tr><tr><td>23</td><td>2460</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"a7d8882f-825c-4b16-a788-6a30f8696cf7\"/>\n\n\n## Land New Data\n\nAs in our previous lesson, we have configured a helper function to process new records into our source table.\n\nRun the cell below to land another batch of data."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f6cbae0a-be64-4a40-9773-de76c5820536","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["DA.data_factory.load()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3e906f3b-8d4e-4a06-8472-631d45ac0335","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Loading the file 02.json to the dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks/tracker/02.json\n"]}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"9e62e0fc-7705-4ee4-8407-07cc908cbfdc\"/>\n\n\nQuery the target table again to see the updated counts for each **`device_id`**."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6a9d096e-a48e-4da9-8e09-814dc166949b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nSELECT *\nFROM device_counts"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"34ffb938-d217-422f-8c99-5ee950c4d855","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["37",1977],["17",1475],["23",2460]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"device_id","type":"\"string\"","metadata":"{}"},{"name":"total_recordings","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>device_id</th><th>total_recordings</th></tr></thead><tbody><tr><td>37</td><td>1977</td></tr><tr><td>17</td><td>1475</td></tr><tr><td>23</td><td>2460</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"9d17fa00-4a3f-48a0-a15e-8ac39df11550\"/>\n\n\n## Clean Up\nFeel free to continue landing new data and exploring the table results with the cells above.\n\nWhen you're finished, run the following cell to stop all active streams and remove created resources before continuing."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1ef2fe59-c940-4d4e-8e85-148ea3ac9f5e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["DA.cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c5b53833-e549-4c2a-8c20-ff0871dfacdb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Resetting the learning environment:\n| stopping the stream \"None\"...(0 seconds)\n| dropping the schema \"jtschopp_k017_dbacademy_dewd\"...(4 seconds)\n| removing the working directory \"dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks\"...(1 seconds)\n\nValidating the locally installed datasets:\n| listing local files...(8 seconds)\n| completed (8 seconds total)\n\n"]}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DE 6.2 - Reasoning about Incremental Data","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":153415606860946,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":153415606860915}},"nbformat":4,"nbformat_minor":0}
