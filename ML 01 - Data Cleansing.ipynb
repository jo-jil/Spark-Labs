{"cells":[{"cell_type":"markdown","source":["<i18n value=\"8c6d3ef3-e44b-4292-a0d3-1aaba0198525\"/>\n\n\n\n# Data Cleansing\n\nWe will be using Spark to do some exploratory data analysis & cleansing of the SF Airbnb rental dataset from <a href=\"http://insideairbnb.com/get-the-data.html\" target=\"_blank\">Inside Airbnb</a>.\n\n<img src=\"https://files.training.databricks.com/images/301/sf.jpg\" style=\"height: 200px; margin: 10px; border: 1px solid #ddd; padding: 10px\"/>\n\n## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) In this lesson you:<br>\n - Impute missing values\n - Identify & remove outliers"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"15f21535-95b1-4b9c-94a4-ba12d57a0372","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%run \"./Includes/Classroom-Setup\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"60dee3a8-10bc-46b5-be5d-39a0183a9747","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"The execution of this command did not finish successfully","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"969507ea-bffc-4255-9a99-2306a594625f\"/>\n\n\n\nLet's load the Airbnb dataset in."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5c77676f-1735-42c0-994f-f1ace7662f9b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["file_path = f\"{DA.paths.datasets}/airbnb/sf-listings/sf-listings-2019-03-06.csv\"\n\nraw_df = spark.read.csv(file_path, header=\"true\", inferSchema=\"true\", multiLine=\"true\", escape='\"')\n\ndisplay(raw_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4c066667-c68d-4884-a425-6b2be87700f2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["raw_df.columns"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2d9670d6-f1c3-482d-9583-ec72f0f4bf8d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"94856418-c319-4915-a73e-5728fcd44101\"/>\n\n\n\nFor the sake of simplicity, only keep certain columns from this dataset. We will talk about feature selection later."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9b54cd30-8d5f-4400-b2a8-6c4f09613ceb","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["columns_to_keep = [\n    \"host_is_superhost\",\n    \"cancellation_policy\",\n    \"instant_bookable\",\n    \"host_total_listings_count\",\n    \"neighbourhood_cleansed\",\n    \"latitude\",\n    \"longitude\",\n    \"property_type\",\n    \"room_type\",\n    \"accommodates\",\n    \"bathrooms\",\n    \"bedrooms\",\n    \"beds\",\n    \"bed_type\",\n    \"minimum_nights\",\n    \"number_of_reviews\",\n    \"review_scores_rating\",\n    \"review_scores_accuracy\",\n    \"review_scores_cleanliness\",\n    \"review_scores_checkin\",\n    \"review_scores_communication\",\n    \"review_scores_location\",\n    \"review_scores_value\",\n    \"price\"\n]\n\nbase_df = raw_df.select(columns_to_keep)\nbase_df.cache().count()\ndisplay(base_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a54033f4-3850-4227-a702-823f8f1132f5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"a12c5a59-ad1c-4542-8695-d822ec10c4ca\"/>\n\n\n \n### Fixing Data Types\n\nTake a look at the schema above. You'll notice that the **`price`** field got picked up as string. For our task, we need it to be a numeric (double type) field. \n\nLet's fix that."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"74a6189c-ef72-45b8-98fe-68003bf2c67a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import col, translate\n\nfixed_price_df = base_df.withColumn(\"price\", translate(col(\"price\"), \"$,\", \"\").cast(\"double\"))\n\ndisplay(fixed_price_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d81339d0-9088-4a10-a5d9-491d836d09f9","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"4ad08138-4563-4a93-b038-801832c9bc73\"/>\n\n\n\n### Summary statistics\n\nTwo options:\n* **`describe`**: count, mean, stddev, min, max\n* **`summary`**: describe + interquartile range (IQR)\n\n**Question:** When to use IQR/median over mean? Vice versa?"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b9e6189a-a6ce-491f-ba07-51ac7dc16875","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["display(fixed_price_df.describe())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ca8419c4-474a-413a-9faa-b8f6109349e2","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["display(fixed_price_df.summary())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a0458462-8901-452d-9d60-120978686497","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"bd55efda-86d0-4584-a6fc-ef4f221b2872\"/>\n\n\n\n### Dbutils Data Summary\n\nWe can also use **`dbutils.data.summarize`** to see more detailed summary statistics and data plots."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"17f74333-e7a4-4a37-b776-64b5e413f617","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["dbutils.data.summarize(fixed_price_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d98e39e8-6d93-46b3-97dc-e1c3d01ea477","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"e9860f92-2fbe-4d23-b728-678a7bb4734e\"/>\n\n\n\n### Getting rid of extreme values\n\nLet's take a look at the *min* and *max* values of the **`price`** column."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"aed16ba2-8d64-4d8a-b92f-e655831ba8ad","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["display(fixed_price_df.select(\"price\").describe())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"328548a5-7b41-444a-bebe-b5a4af33b5ce","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"4a8fe21b-1dac-4edf-a0a3-204f170b05c9\"/>\n\n\n\nThere are some super-expensive listings, but it's up to the SME (Subject Matter Experts) to decide what to do with them. We can certainly filter the \"free\" Airbnbs though.\n\nLet's see first how many listings we can find where the *price* is zero."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"fc373966-0077-4e08-ab3f-2557618e1e2e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["fixed_price_df.filter(col(\"price\") == 0).count()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dad27632-1154-4b26-82a3-74d2bcb9f408","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"bf195d9b-ea4d-4a3e-8b61-372be8eec327\"/>\n\n\n\nNow only keep rows with a strictly positive *price*."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5de42fcd-e56b-40e2-9c3b-d8968e3bfb1d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["pos_prices_df = fixed_price_df.filter(col(\"price\") > 0)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1d48d86a-fb8f-449c-9c6d-aaaf7f961813","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"dc8600db-ebd1-4110-bfb1-ce555bc95245\"/>\n\n\n\nLet's take a look at the *min* and *max* values of the *minimum_nights* column:"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f7e68c1a-89cf-4aa7-9360-f527e2c8d3b3","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["display(pos_prices_df.select(\"minimum_nights\").describe())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1358934f-d6b4-483b-ab3e-c30ff4e5b70d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["display(pos_prices_df\n        .groupBy(\"minimum_nights\").count()\n        .orderBy(col(\"count\").desc(), col(\"minimum_nights\"))\n       )"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"96f4dc07-6e32-4876-bb94-0d9e0f61685b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"5aa4dfa8-d9a1-42e2-9060-a5dcc3513a0d\"/>\n\n\n\nA minimum stay of one year seems to be a reasonable limit here. Let's filter out those records where the *minimum_nights* is greater then 365."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1b9a5fab-3703-4b7d-b6d8-1546a65048f8","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["min_nights_df = pos_prices_df.filter(col(\"minimum_nights\") <= 365)\n\ndisplay(min_nights_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"05c8b37e-b00b-49d9-92e3-8dbb0a7aed8b","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"25a35390-d716-43ad-8f51-7e7690e1c913\"/>\n\n\n\n### Handling Null Values\n\nThere are a lot of different ways to handle null values. Sometimes, null can actually be a key indicator of the thing you are trying to predict (e.g. if you don't fill in certain portions of a form, probability of it getting approved decreases).\n\nSome ways to handle nulls:\n* Drop any records that contain nulls\n* Numeric:\n  * Replace them with mean/median/zero/etc.\n* Categorical:\n  * Replace them with the mode\n  * Create a special category for null\n* Use techniques like ALS (Alternating Least Squares) which are designed to impute missing values\n  \n**If you do ANY imputation techniques for categorical/numerical features, you MUST include an additional field specifying that field was imputed.**\n\nSparkML's Imputer (covered below) does not support imputation for categorical features."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ed474c49-68ba-4044-b96f-46ce4d5b1294","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"83e56fca-ce6d-4e3c-8042-0c1c7b9eaa5a\"/>\n\n\n\n### Impute: Cast to Double\n\nSparkML's <a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.Imputer.html?highlight=imputer#pyspark.ml.feature.Imputer\" target=\"_blank\">Imputer </a> requires all fields be of type double. Let's cast all integer fields to double."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f0d0dd38-b245-4f54-b75a-cfc739658d4f","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import col\nfrom pyspark.sql.types import IntegerType\n\ninteger_columns = [x.name for x in min_nights_df.schema.fields if x.dataType == IntegerType()]\ndoubles_df = min_nights_df\n\nfor c in integer_columns:\n    doubles_df = doubles_df.withColumn(c, col(c).cast(\"double\"))\n\ncolumns = \"\\n - \".join(integer_columns)\nprint(f\"Columns converted from Integer to Double:\\n - {columns}\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"36885555-5b1e-4145-8a3c-dd6fd0d3f62c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"69b58107-82ad-4cec-8984-028a5df1b69e\"/>\n\n\n\nAdd a dummy column to denote presence of null values before imputing."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"226ea61f-6d5e-410d-8ce0-adf9259cdb2c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.sql.functions import when\n\nimpute_cols = [\n    \"bedrooms\",\n    \"bathrooms\",\n    \"beds\", \n    \"review_scores_rating\",\n    \"review_scores_accuracy\",\n    \"review_scores_cleanliness\",\n    \"review_scores_checkin\",\n    \"review_scores_communication\",\n    \"review_scores_location\",\n    \"review_scores_value\"\n]\n\nfor c in impute_cols:\n    doubles_df = doubles_df.withColumn(c + \"_na\", when(col(c).isNull(), 1.0).otherwise(0.0))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c31a86a6-454f-4113-a425-56bd7b6fffc7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"code","source":["display(doubles_df.describe())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5a210dcb-2fe8-48b9-b5ca-2ec21edddd00","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"c88f432d-1252-4acc-8c91-4834c00da789\"/>\n\n\n\n### Transformers and Estimators\n\nSpark ML standardizes APIs for machine learning algorithms to make it easier to combine multiple algorithms into a single pipeline, or workflow. Let's cover two key concepts introduced by the Spark ML API: **`transformers`** and **`estimators`**.\n\n**Transformer**: Transforms one DataFrame into another DataFrame. It accepts a DataFrame as input, and returns a new DataFrame with one or more columns appended to it. Transformers do not learn any parameters from your data and simply apply rule-based transformations. It has a **`.transform()`** method.\n\n**Estimator**: An algorithm which can be fit on a DataFrame to produce a Transformer. E.g., a learning algorithm is an Estimator which trains on a DataFrame and produces a model. It has a **`.fit()`** method because it learns (or \"fits\") parameters from your DataFrame."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7032abe7-8e96-489b-a2a3-a94034a19a4b","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["from pyspark.ml.feature import Imputer\n\nimputer = Imputer(strategy=\"median\", inputCols=impute_cols, outputCols=impute_cols)\n\nimputer_model = imputer.fit(doubles_df)\nimputed_df = imputer_model.transform(doubles_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b59f3892-71fb-48f0-b7e9-192f41c8a694","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"4df06e83-27e6-4cc6-b66d-883317b2a7eb\"/>\n\n\n\nOK, our data is cleansed now. Let's save this DataFrame to Delta so that we can start building models with it."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2df3dc87-f8ef-4cef-a7c5-abccd21dfe23","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["imputed_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{DA.paths.working_dir}/imputed_results\")"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"94a273ca-2d79-407b-9d24-4a098983a2d7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"","errorSummary":"Command skipped","metadata":{},"errorTraceType":"ansi","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/plain":[""]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"ML 01 - Data Cleansing","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":153415606862635}},"nbformat":4,"nbformat_minor":0}
