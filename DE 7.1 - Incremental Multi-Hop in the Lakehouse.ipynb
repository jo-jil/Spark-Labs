{"cells":[{"cell_type":"markdown","source":["<i18n value=\"c84bb70e-0f3a-4cb9-a8b4-882200c7c940\"/>\n\n\n# Incremental Multi-Hop in the Lakehouse\n\nNow that we have a better understanding of how to work with incremental data processing by combining Structured Streaming APIs and Spark SQL, we can explore the tight integration between Structured Streaming and Delta Lake.\n\n\n\n## Learning Objectives\nBy the end of this lesson, you should be able to:\n* Describe Bronze, Silver, and Gold tables\n* Create a Delta Lake multi-hop pipeline"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"df4ea592-ead6-4a06-8c92-5dae4355de23","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"8f7d994a-fe1f-4628-825e-30c35b9ff187\"/>\n\n\n## Incremental Updates in the Lakehouse\n\nDelta Lake allows users to easily combine streaming and batch workloads in a unified multi-hop pipeline. Each stage of the pipeline represents a state of our data valuable to driving core use cases within the business. Because all data and metadata lives in object storage in the cloud, multiple users and applications can access data in near-real time, allowing analysts to access the freshest data as it's being processed.\n\n![](https://files.training.databricks.com/images/sslh/multi-hop-simple.png)\n\n- **Bronze** tables contain raw data ingested from various sources (JSON files, RDBMS data,  IoT data, to name a few examples).\n\n- **Silver** tables provide a more refined view of our data. We can join fields from various bronze tables to enrich streaming records, or update account statuses based on recent activity.\n\n- **Gold** tables provide business level aggregates often used for reporting and dashboarding. This would include aggregations such as daily active website users, weekly sales per store, or gross revenue per quarter by department. \n\nThe end outputs are actionable insights, dashboards and reports of business metrics.\n\nBy considering our business logic at all steps of the ETL pipeline, we can ensure that storage and compute costs are optimized by reducing unnecessary duplication of data and limiting ad hoc querying against full historic data.\n\nEach stage can be configured as a batch or streaming job, and ACID transactions ensure that we succeed or fail completely."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6ca7c447-0132-4c58-803b-6d175628fa52","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"9008b325-00b1-41a3-bc43-9c693bade882\"/>\n\n\n## Datasets Used\n\nThis demo uses simplified artificially generated medical data. The schema of our two datasets is represented below. Note that we will be manipulating these schema during various steps.\n\n#### Recordings\nThe main dataset uses heart rate recordings from medical devices delivered in the JSON format. \n\n| Field | Type |\n| --- | --- |\n| device_id | int |\n| mrn | long |\n| time | double |\n| heartrate | double |\n\n#### PII\nThese data will later be joined with a static table of patient information stored in an external system to identify patients by name.\n\n| Field | Type |\n| --- | --- |\n| mrn | long |\n| name | string |"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"45b3f0be-c3d4-48a6-abea-f9e408f1d03e","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"7b621659-663d-4fea-b26c-5eefdf4d025a\"/>\n\n\n## Getting Started\n\nRun the following cell to configure the lab environment."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3e28d8f5-6926-465d-a214-996c4e1d3e30","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup-07.1"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"88f5f3d6-196c-40f1-b4f8-3866bc68fd8c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Python interpreter will be restarted.\nPython interpreter will be restarted.\n"]},{"output_type":"stream","output_type":"stream","name":"stdout","text":["\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/data-engineering-with-databricks/v02\"\n\nValidating the locally installed datasets:\n| listing local files...(7 seconds)\n| completed (7 seconds total)\n\nCreating & using the schema \"jtschopp_k017_dbacademy_dewd\"...(0 seconds)\nPredefined tables in \"jtschopp_k017_dbacademy_dewd\":\n| -none-\n\nPredefined paths variables:\n| DA.paths.working_dir:           dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks\n| DA.paths.user_db:               dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks/database.db\n| DA.paths.datasets:              dbfs:/mnt/dbacademy-datasets/data-engineering-with-databricks/v02\n| DA.paths.checkpoints:           dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks/_checkpoints\n| DA.paths.data_landing_location: dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks/source/tracker\n\nSetup completed (9 seconds)\n"]}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"045c9907-e803-4506-8e69-4e370f06cd1d\"/>\n\n\n## Data Simulator\nDatabricks Auto Loader can automatically process files as they land in your cloud object stores. \n\nTo simulate this process, you will be asked to run the following operation several times throughout the course."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"d2220f0e-22cb-4d8a-a41b-8a65869582d7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["DA.data_factory.load()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6574b645-6dd0-405e-954c-c505220c81d8","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Loading the file 01.json to the dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks/source/tracker/01.json\n"]}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"d5d9393e-0a91-41f5-95f3-82f1be290add\"/>\n\n\n## Bronze Table: Ingesting Raw JSON Recordings\n\nBelow, we configure a read on a raw JSON source using Auto Loader with schema inference.\n\nNote that while you need to use the Spark DataFrame API to set up an incremental read, once configured you can immediately register a temp view to leverage Spark SQL for streaming transformations on your data.\n\n**NOTE**: For a JSON data source, Auto Loader will default to inferring each column as a string. Here, we demonstrate specifying the data type for the **`time`** column using the **`cloudFiles.schemaHints`** option. Note that specifying improper types for a field will result in null values."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"835fcee1-9ee3-47a6-b4d3-011bddf0726a","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["(spark.readStream\n    .format(\"cloudFiles\")\n    .option(\"cloudFiles.format\", \"json\")\n    .option(\"cloudFiles.schemaHints\", \"time DOUBLE\")\n    .option(\"cloudFiles.schemaLocation\", f\"{DA.paths.checkpoints}/bronze\")\n    .load(DA.paths.data_landing_location)\n    .createOrReplaceTempView(\"recordings_raw_temp\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"15954478-0d4f-4a1a-a8fa-11e6696b4a90","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"7fdec7ea-277e-4df4-911b-b2a4d3761b6a\"/>\n\n\nHere, we'll enrich our raw data with additional metadata describing the source file and the time it was ingested. This additional metadata can be ignored during downstream processing while providing useful information for troubleshooting errors if corrupt data is encountered."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1e548a58-0bfd-4562-9e64-b4d3c4125b71","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMPORARY VIEW recordings_bronze_temp AS (\n  SELECT *, current_timestamp() receipt_time, input_file_name() source_file\n  FROM recordings_raw_temp\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"a08d6711-93d6-445f-8666-40b994af7608","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"6f60f3aa-65ff-4204-9cf8-00f456d4497b\"/>\n\n\nThe code below passes our enriched raw data back to PySpark API to process an incremental write to a Delta Lake table."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2b5e4e1c-5072-48f0-9a95-525d65cfa2be","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["(spark.table(\"recordings_bronze_temp\")\n      .writeStream\n      .format(\"delta\")\n      .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/bronze\")\n      .outputMode(\"append\")\n      .table(\"bronze\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1a80d9fe-8a66-4308-95c4-a2d4d4f5fbfb","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[11]: <pyspark.sql.streaming.query.StreamingQuery at 0x7f0830050550>"]}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"6fd28dc4-1516-4f6a-8478-290d366a342c\"/>\n\n\nTrigger another file arrival with the following cell and you'll see the changes immediately detected by the streaming query you've written."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"ccc7a4e7-3511-4939-8dac-7f41a24b898c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["DA.data_factory.load()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5ebc7873-ed85-439a-8035-09f775d6a815","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Loading the file 02.json to the dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks/source/tracker/02.json\n"]}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"4d7848cc-ecff-474d-be27-21717d9f08d1\"/>\n\n\n### Load Static Lookup Table\nThe ACID guarantees that Delta Lake brings to your data are managed at the table level, ensuring that only fully successfully commits are reflected in your tables. If you choose to merge these data with other data sources, be aware of how those sources version data and what sort of consistency guarantees they have.\n\nIn this simplified demo, we are loading a static CSV file to add patient data to our recordings. In production, we could use Databricks' <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/auto-loader.html\" target=\"_blank\">Auto Loader</a> feature to keep an up-to-date view of these data in our Delta Lake."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"da50b0e6-a790-4afe-815f-4d0fb950b6df","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["(spark.read\n      .format(\"csv\")\n      .schema(\"mrn STRING, name STRING\")\n      .option(\"header\", True)\n      .load(f\"{DA.paths.datasets}/healthcare/patient/patient_info.csv\")\n      .createOrReplaceTempView(\"pii\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"a806ea44-74cc-468d-80b9-9b329c2f213c","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nSELECT * FROM pii"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"71744215-fbd4-453d-b552-e71bd3ec9e7e","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["23940128","Caitlin Garcia"],["18064290","Anthony Perez"],["95384990","Tanya Diaz"],["53057176","Autumn Calderon"],["96005424","Ronald Smith"],["70379340","Robert Vincent"],["38299723","Cynthia Figueroa"],["41675882","Crystal Ho"],["21067668","John Estes"],["88104185","George Wagner"],["46400860","Melissa Martinez"],["53962192","Sharon Brewer"],["55827205","Joshua Perkins"],["71425339","Michelle Carter"],["18477029","Sean Brown"],["40580129","Nicholas Spears"],["19778195","Dr. Amanda Baxter"],["96965325","Gabriela Gibson"],["32512514","Mark Harris"],["65300842","Samuel Hughes"],["46722881","Rachel Contreras"],["30687748","Troy Davis"],["84682617","Kyle Cruz"],["27831169","Ashley Schmidt"],["89287656","Mary Adams"],["55527081","George King"],["52804177","Lynn Russell"],["77385574","Michael Maxwell"],["15902097","John Smith"],["10288418","Valerie Reese"],["90389540","Sarah Kennedy"],["43830805","William Gomez Jr."],["20793791","Valerie Garcia"],["97376381","Joshua Harris"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"mrn","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>mrn</th><th>name</th></tr></thead><tbody><tr><td>23940128</td><td>Caitlin Garcia</td></tr><tr><td>18064290</td><td>Anthony Perez</td></tr><tr><td>95384990</td><td>Tanya Diaz</td></tr><tr><td>53057176</td><td>Autumn Calderon</td></tr><tr><td>96005424</td><td>Ronald Smith</td></tr><tr><td>70379340</td><td>Robert Vincent</td></tr><tr><td>38299723</td><td>Cynthia Figueroa</td></tr><tr><td>41675882</td><td>Crystal Ho</td></tr><tr><td>21067668</td><td>John Estes</td></tr><tr><td>88104185</td><td>George Wagner</td></tr><tr><td>46400860</td><td>Melissa Martinez</td></tr><tr><td>53962192</td><td>Sharon Brewer</td></tr><tr><td>55827205</td><td>Joshua Perkins</td></tr><tr><td>71425339</td><td>Michelle Carter</td></tr><tr><td>18477029</td><td>Sean Brown</td></tr><tr><td>40580129</td><td>Nicholas Spears</td></tr><tr><td>19778195</td><td>Dr. Amanda Baxter</td></tr><tr><td>96965325</td><td>Gabriela Gibson</td></tr><tr><td>32512514</td><td>Mark Harris</td></tr><tr><td>65300842</td><td>Samuel Hughes</td></tr><tr><td>46722881</td><td>Rachel Contreras</td></tr><tr><td>30687748</td><td>Troy Davis</td></tr><tr><td>84682617</td><td>Kyle Cruz</td></tr><tr><td>27831169</td><td>Ashley Schmidt</td></tr><tr><td>89287656</td><td>Mary Adams</td></tr><tr><td>55527081</td><td>George King</td></tr><tr><td>52804177</td><td>Lynn Russell</td></tr><tr><td>77385574</td><td>Michael Maxwell</td></tr><tr><td>15902097</td><td>John Smith</td></tr><tr><td>10288418</td><td>Valerie Reese</td></tr><tr><td>90389540</td><td>Sarah Kennedy</td></tr><tr><td>43830805</td><td>William Gomez Jr.</td></tr><tr><td>20793791</td><td>Valerie Garcia</td></tr><tr><td>97376381</td><td>Joshua Harris</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"00ed3fc7-7c17-44f0-b56f-5e824a72bd9c\"/>\n\n\n## Silver Table: Enriched Recording Data\nAs a second hop in our silver level, we will do the follow enrichments and checks:\n- Our recordings data will be joined with the PII to add patient names\n- The time for our recordings will be parsed to the format **`'yyyy-MM-dd HH:mm:ss'`** to be human-readable\n- We will exclude heart rates that are <= 0, as we know that these either represent the absence of the patient or an error in transmission"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5095d118-ffe9-4b8c-a7b3-4d4fb11b07e4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["(spark.readStream\n  .table(\"bronze\")\n  .createOrReplaceTempView(\"bronze_tmp\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b7ce0804-4d2f-46c5-a9e9-d3263a7d2bbe","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMPORARY VIEW recordings_w_pii AS (\n  SELECT device_id, a.mrn, b.name, cast(from_unixtime(time, 'yyyy-MM-dd HH:mm:ss') AS timestamp) time, heartrate\n  FROM bronze_tmp a\n  INNER JOIN pii b\n  ON a.mrn = b.mrn\n  WHERE heartrate > 0)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"c76b78af-45b1-46a3-b2a3-34d6b37067e4","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["(spark.table(\"recordings_w_pii\")\n      .writeStream\n      .format(\"delta\")\n      .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/recordings_enriched\")\n      .outputMode(\"append\")\n      .table(\"recordings_enriched\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"db3568a3-21ee-499f-b6af-a936d412ea8f","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[17]: <pyspark.sql.streaming.query.StreamingQuery at 0x7f08300354f0>"]}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"f7f66dc8-f5b5-4682-bfb6-e97aab650874\"/>\n\n\nTrigger another new file and wait for it propagate through both previous queries."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c9fa2e55-1dd0-4d9f-b4d9-d850931c65a7","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nSELECT COUNT(*) FROM recordings_enriched"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"877f3cdf-4cea-48d6-a5e3-6f655a7c2512","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[0]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"count(1)","type":"\"long\"","metadata":"{\"__autoGeneratedAlias\":\"true\"}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>0</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["DA.data_factory.load()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"9e264acb-bbf1-47c5-9ecf-5444281d3045","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Loading the file 03.json to the dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks/source/tracker/03.json\n"]}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"d6a2ecd9-043e-4488-8a70-3ee3389cf681\"/>\n\n\n## Gold Table: Daily Averages\n\nHere we read a stream of data from **`recordings_enriched`** and write another stream to create an aggregate gold table of daily averages for each patient."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"95ccc1e3-435a-4250-b059-7c75ccee20fb","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["(spark.readStream\n  .table(\"recordings_enriched\")\n  .createOrReplaceTempView(\"recordings_enriched_temp\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0ba7eb8a-cef0-46c7-be9e-1385002ef1ef","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%sql\nCREATE OR REPLACE TEMP VIEW patient_avg AS (\n  SELECT mrn, name, mean(heartrate) avg_heartrate, date_trunc(\"DD\", time) date\n  FROM recordings_enriched_temp\n  GROUP BY mrn, name, date_trunc(\"DD\", time))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"7743ca95-7485-4046-98c1-4859af6ac80d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"de6370ea-e1a0-4212-98eb-53fd012e73b0\"/>\n\n\nNote that we're using **`.trigger(availableNow=True)`** below. This provides us the ability to continue to use the strengths of Structured Streaming while triggering this job one-time to process all available data in micro-batches. To recap, these strengths include:\n- exactly once end-to-end fault tolerant processing\n- automatic detection of changes in upstream data sources\n\nIf we know the approximate rate at which our data grows, we can appropriately size the cluster we schedule for this job to ensure fast, cost-effective processing. The customer will be able to evaluate how much updating this final aggregate view of their data costs and make informed decisions about how frequently this operation needs to be run.\n\nDownstream processes subscribing to this table do not need to re-run any expensive aggregations. Rather, files just need to be de-serialized and then queries based on included fields can quickly be pushed down against this already-aggregated source."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"5cbf5814-6b90-4c5c-9e6d-ddadb291ed98","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["(spark.table(\"patient_avg\")\n      .writeStream\n      .format(\"delta\")\n      .outputMode(\"complete\")\n      .option(\"checkpointLocation\", f\"{DA.paths.checkpoints}/daily_avg\")\n      .trigger(availableNow=True)\n      .table(\"daily_patient_avg\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"22393e5b-7d49-4338-98e0-7ef4481bfb1a","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Out[22]: <pyspark.sql.streaming.query.StreamingQuery at 0x7f083005ceb0>"]}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"5ffbd353-850f-431e-8455-827f87cad2ca\"/>\n\n\n#### Important Considerations for complete Output with Delta\n\nWhen using **`complete`** output mode, we rewrite the entire state of our table each time our logic runs. While this is ideal for calculating aggregates, we **cannot** read a stream from this directory, as Structured Streaming assumes data is only being appended in the upstream logic.\n\n**NOTE**: Certain options can be set to change this behavior, but have other limitations attached. For more details, refer to <a href=\"https://docs.databricks.com/delta/delta-streaming.html#ignoring-updates-and-deletes\" target=\"_blank\">Delta Streaming: Ignoring Updates and Deletes</a>.\n\nThe gold Delta table we have just registered will perform a static read of the current state of the data each time we run the following query."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"3821b7bc-26e1-4727-8afb-106c27525c89","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nSELECT * FROM daily_patient_avg"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"637cec12-783d-4b43-8b02-73577c043d91","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"mrn","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"avg_heartrate","type":"\"double\"","metadata":"{}"},{"name":"date","type":"\"timestamp\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>mrn</th><th>name</th><th>avg_heartrate</th><th>date</th></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"bcca7247-9716-44ed-8424-e72170f0a2dc\"/>\n\n\nNote the above table includes all days for all users. If the predicates for our ad hoc queries match the data encoded here, we can push down our predicates to files at the source and very quickly generate more limited aggregate views."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b9e955a6-a6c1-43b6-95c8-6c69d90009f0","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\nSELECT * \nFROM daily_patient_avg\nWHERE date BETWEEN \"2020-01-17\" AND \"2020-01-31\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"e8a94ce4-9295-4cb4-8681-16b592b75353","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"mrn","type":"\"string\"","metadata":"{}"},{"name":"name","type":"\"string\"","metadata":"{}"},{"name":"avg_heartrate","type":"\"double\"","metadata":"{}"},{"name":"date","type":"\"timestamp\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>mrn</th><th>name</th><th>avg_heartrate</th><th>date</th></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"0822f785-38af-4b30-9154-8d82eb9fe000\"/>\n\n\n## Process Remaining Records\nThe following cell will land additional files for the rest of 2020 in your source directory. You'll be able to see these process through the first 3 tables in your Delta Lake, but will need to re-run your final query to update your **`daily_patient_avg`** table, since this query uses the trigger available now syntax."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f289711e-3b76-45f7-9b05-18bc2bf27725","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["DA.data_factory.load(continuous=True)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6a2856b4-3a95-4a19-b8d6-a63b4889aea7","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Loading the file 04.json to the dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks/source/tracker/04.json\nLoading the file 05.json to the dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks/source/tracker/05.json\nLoading the file 06.json to the dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks/source/tracker/06.json\nLoading the file 07.json to the dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks/source/tracker/07.json\nLoading the file 08.json to the dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks/source/tracker/08.json\nLoading the file 09.json to the dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks/source/tracker/09.json\nLoading the file 10.json to the dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks/source/tracker/10.json\nLoading the file 11.json to the dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks/source/tracker/11.json\nLoading the file 12.json to the dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks/source/tracker/12.json\n"]}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"f1f576bc-2b5d-46cf-9acb-6c7c4807c1af\"/>\n\n\n## Wrapping Up\n\nFinally, make sure all streams are stopped."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"4d71531b-1f62-4561-865a-abe9a07dee7c","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["DA.cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b9c780df-618d-4e79-9394-74ed15583ec6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Resetting the learning environment:\n| stopping the stream \"None\"...(1 seconds)\n| stopping the stream \"None\"...(0 seconds)\n| stopping the stream \"None\"...(0 seconds)\n| dropping the schema \"jtschopp_k017_dbacademy_dewd\"...(2 seconds)\n| removing the working directory \"dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks\"...(1 seconds)\n\nValidating the locally installed datasets:\n| listing local files...(7 seconds)\n| completed (7 seconds total)\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"82928cc5-5e2b-4368-90bd-dff62a27ff12\"/>\n\n\n## Summary\n\nDelta Lake and Structured Streaming combine to provide near real-time analytic access to data in the lakehouse."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"6e6d776c-cfe7-4b41-b8fa-f07937cb51d1","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"e60b0dac-92ed-4480-a969-d0568ce83494\"/>\n\n\n## Additional Topics & Resources\n\n* <a href=\"https://docs.databricks.com/delta/delta-streaming.html\" target=\"_blank\">Table Streaming Reads and Writes</a>\n* <a href=\"https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html\" target=\"_blank\">Structured Streaming Programming Guide</a>\n* <a href=\"https://www.youtube.com/watch?v=rl8dIzTpxrI\" target=\"_blank\">A Deep Dive into Structured Streaming</a> by Tathagata Das. This is an excellent video describing how Structured Streaming works.\n* <a href=\"https://databricks.com/glossary/lambda-architecture\" target=\"_blank\">Lambda Architecture</a>\n* <a href=\"https://bennyaustin.wordpress.com/2010/05/02/kimball-and-inmon-dw-models/#\" target=\"_blank\">Data Warehouse Models</a>\n* <a href=\"http://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html\" target=\"_blank\">Create a Kafka Source Stream</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"0ee3fac7-069f-41cb-b0f0-183bfcf89891","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DE 7.1 - Incremental Multi-Hop in the Lakehouse","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":153415606860888,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":153415606860854}},"nbformat":4,"nbformat_minor":0}
