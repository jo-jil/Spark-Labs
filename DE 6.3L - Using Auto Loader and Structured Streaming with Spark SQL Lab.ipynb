{"cells":[{"cell_type":"markdown","source":["<i18n value=\"e6dedae8-1335-494e-acdf-4a1906f8c826\"/>\n\n\n# Using Auto Loader and Structured Streaming with Spark SQL\n\n## Learning Objectives\nBy the end of this lab, you should be able to:\n* Ingest data using Auto Loader\n* Aggregate streaming data\n* Stream data to a Delta table"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"c4b0d9b6-6444-462d-9b74-b2c1a6b91acf","inputWidgets":{},"title":""}}},{"cell_type":"markdown","source":["<i18n value=\"ab5018b7-17b9-4f66-a32d-9c86860f6f30\"/>\n\n\n## Setup\nRun the following script to setup necessary variables and clear out past runs of this notebook. Note that re-executing this cell will allow you to start the lab over."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"f01e6358-87ea-4274-b5a8-a049a951c08d","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%run ../Includes/Classroom-Setup-06.3L"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2b2568be-bbb2-47bf-89b0-5998724ac2f6","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Python interpreter will be restarted.\nPython interpreter will be restarted.\n"]},{"output_type":"stream","output_type":"stream","name":"stdout","text":["Resetting the learning environment:\n| dropping the schema \"jtschopp_k017_dbacademy_dewd\"...(1 seconds)\n| removing the working directory \"dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks\"...(0 seconds)\n\nSkipping install of existing datasets to \"dbfs:/mnt/dbacademy-datasets/data-engineering-with-databricks/v02\"\n\nValidating the locally installed datasets:\n| listing local files...(8 seconds)\n| completed (8 seconds total)\n\nCreating & using the schema \"jtschopp_k017_dbacademy_dewd\"...(1 seconds)\nPredefined tables in \"jtschopp_k017_dbacademy_dewd\":\n| -none-\n\nPredefined paths variables:\n| DA.paths.working_dir: dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks\n| DA.paths.user_db:     dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks/database.db\n| DA.paths.datasets:    dbfs:/mnt/dbacademy-datasets/data-engineering-with-databricks/v02\n| DA.paths.checkpoints: dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks/_checkpoints\n\nSetup completed (12 seconds)\n"]}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"03347519-151b-4304-8cda-1cbd91af0737\"/>\n\n\n\n## Configure Streaming Read\n\nThis lab uses a collection of customer-related CSV data from the **retail-org/customers** dataset.\n\nRead this data using <a href=\"https://docs.databricks.com/spark/latest/structured-streaming/auto-loader.html\" target=\"_blank\">Auto Loader</a> using its schema inference (use **`customers_checkpoint_path`** to store the schema info). Create a streaming temporary view called **`customers_raw_temp`**."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cfbb4e47-d86d-416b-b9c0-a168938d4f95","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# TODO\ndataset_source = f\"{DA.paths.datasets}/retail-org/customers/\"\ncustomers_checkpoint_path = f\"{DA.paths.checkpoints}/customers\"\n\n(spark\n  .readStream\n  .format(\"cloudFiles\")\n  .option(\"cloudFiles.format\", \"csv\")\n  .option(\"cloudFiles.schemaLocation\", customers_checkpoint_path)\n  .load(dataset_source)\n  .createOrReplaceTempView(\"customers_raw_temp\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"b04a5ef9-7e91-4f9d-82b3-fabbb6c79538","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import Row\nassert Row(tableName=\"customers_raw_temp\", isTemporary=True) in spark.sql(\"show tables\").select(\"tableName\", \"isTemporary\").collect(), \"Table not present or not temporary\"\nassert spark.table(\"customers_raw_temp\").dtypes ==  [('customer_id', 'string'),\n ('tax_id', 'string'),\n ('tax_code', 'string'),\n ('customer_name', 'string'),\n ('state', 'string'),\n ('city', 'string'),\n ('postcode', 'string'),\n ('street', 'string'),\n ('number', 'string'),\n ('unit', 'string'),\n ('region', 'string'),\n ('district', 'string'),\n ('lon', 'string'),\n ('lat', 'string'),\n ('ship_to_address', 'string'),\n ('valid_from', 'string'),\n ('valid_to', 'string'),\n ('units_purchased', 'string'),\n ('loyalty_segment', 'string'),\n ('_rescued_data', 'string')], \"Incorrect Schema\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"931f8246-3024-4ff4-bdb6-378883aa2e71","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"4582665f-8192-4751-83f8-8ae1a4d55f22\"/>\n\n\n\n## Define a streaming aggregation\n\nUsing CTAS syntax, define a new streaming view called **`customer_count_by_state_temp`** that counts the number of customers per **`state`**, in a field called **`customer_count`**."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"7c7b20e6-727a-49b5-97f1-81fc9897ce40","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\n-- TODO\n\nCREATE OR REPLACE TEMPORARY VIEW customer_count_by_state_temp AS\nSELECT\n  state,\n  count(customer_id) AS customer_count\n  FROM customers_raw_temp\n  GROUP BY\n  state"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"cc0a54a8-4c69-4d07-8bd1-2b5add940e77","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr></tr></thead><tbody></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["assert Row(tableName=\"customer_count_by_state_temp\", isTemporary=True) in spark.sql(\"show tables\").select(\"tableName\", \"isTemporary\").collect(), \"Table not present or not temporary\"\nassert spark.table(\"customer_count_by_state_temp\").dtypes == [('state', 'string'), ('customer_count', 'bigint')], \"Incorrect Schema\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"1b1de67c-0181-472d-8ce2-36c972ebb6c3","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"bef919d7-d681-4233-8da5-39ca94c49a8b\"/>\n\n\n\n## Write aggregated data to a Delta table\n\nStream data from the **`customer_count_by_state_temp`** view to a Delta table called **`customer_count_by_state`**."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8ea3fc89-50d8-4724-9114-df8c88aef7de","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["# TODO\ncustomers_count_checkpoint_path = f\"{DA.paths.checkpoints}/customers_count\"\n\nquery = (spark.table(\"customer_count_by_state_temp\")\n              .writeStream\n              .format(\"delta\")\n              .option(\"checkpointLocation\", customers_count_checkpoint_path)\n              .outputMode(\"complete\")\n              .table(\"customer_count_by_state\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8ea28874-a233-47cb-9d39-0c258aa83b2a","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["DA.block_until_stream_is_ready(query)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"10ca13f9-457c-407b-a044-5a9e70f0b57c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Processed 0 of 2 batches...\nProcessed 1 of 2 batches...\nProcessed 1 of 2 batches...\nProcessed 1 of 2 batches...\nProcessed 1 of 2 batches...\nProcessed 2 of 2 batches...\nThe stream is now active with 2 batches having been processed.\n"]}],"execution_count":0},{"cell_type":"code","source":["assert Row(tableName=\"customer_count_by_state\", isTemporary=False) in spark.sql(\"show tables\").select(\"tableName\", \"isTemporary\").collect(), \"Table not present or not temporary\"\nassert spark.table(\"customer_count_by_state\").dtypes == [('state', 'string'), ('customer_count', 'bigint')], \"Incorrect Schema\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"cb897cc8-93a6-454a-9314-35fb10213c3b","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"f74f262f-10c4-4f2f-84d6-f69e56c54ac6\"/>\n\n\n\n## Query the results\n\nQuery the **`customer_count_by_state`** table (this will not be a streaming query). Plot the results as a bar graph and also using the map plot."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"784bb9a8-8b51-4c00-9de3-70467943c099","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["%sql\n-- TODO\nSELECT * FROM customer_count_by_state"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{"implicitDf":true},"nuid":"e1d8a577-42f7-4af9-a1f7-3520dbb0fab5","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["MT",203],["PA",930],["NH",2],["AK",37],["WI",992],["OH",1914],["WA",669],["UT",416],["ND",61],["ME",339],["DE",37],["CO",720],["FL",2526],["TX",567],["NV",40],["AR",11],["MA",1889],["VT",157],["HI",65],["AL",65],["RI",223],["LA",111],["WY",26],["OK",30],["KS",671],["CA",2903],["KY",95],["IN",1105],["WV",5],["NY",3417],["MN",622],["MD",419],["VA",710],["NE",41],["CT",7],["MI",1090],["IA",74],["NM",23],["MS",18],["NC",1016],["MO",507],["OR",433],["SD",26],["TN",140],["SC",173],["ID",32],["AZ",600],["GA",256],["DC",21],["IL",876],["NJ",1503]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"state","type":"\"string\"","metadata":"{}"},{"name":"customer_count","type":"\"long\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>state</th><th>customer_count</th></tr></thead><tbody><tr><td>MT</td><td>203</td></tr><tr><td>PA</td><td>930</td></tr><tr><td>NH</td><td>2</td></tr><tr><td>AK</td><td>37</td></tr><tr><td>WI</td><td>992</td></tr><tr><td>OH</td><td>1914</td></tr><tr><td>WA</td><td>669</td></tr><tr><td>UT</td><td>416</td></tr><tr><td>ND</td><td>61</td></tr><tr><td>ME</td><td>339</td></tr><tr><td>DE</td><td>37</td></tr><tr><td>CO</td><td>720</td></tr><tr><td>FL</td><td>2526</td></tr><tr><td>TX</td><td>567</td></tr><tr><td>NV</td><td>40</td></tr><tr><td>AR</td><td>11</td></tr><tr><td>MA</td><td>1889</td></tr><tr><td>VT</td><td>157</td></tr><tr><td>HI</td><td>65</td></tr><tr><td>AL</td><td>65</td></tr><tr><td>RI</td><td>223</td></tr><tr><td>LA</td><td>111</td></tr><tr><td>WY</td><td>26</td></tr><tr><td>OK</td><td>30</td></tr><tr><td>KS</td><td>671</td></tr><tr><td>CA</td><td>2903</td></tr><tr><td>KY</td><td>95</td></tr><tr><td>IN</td><td>1105</td></tr><tr><td>WV</td><td>5</td></tr><tr><td>NY</td><td>3417</td></tr><tr><td>MN</td><td>622</td></tr><tr><td>MD</td><td>419</td></tr><tr><td>VA</td><td>710</td></tr><tr><td>NE</td><td>41</td></tr><tr><td>CT</td><td>7</td></tr><tr><td>MI</td><td>1090</td></tr><tr><td>IA</td><td>74</td></tr><tr><td>NM</td><td>23</td></tr><tr><td>MS</td><td>18</td></tr><tr><td>NC</td><td>1016</td></tr><tr><td>MO</td><td>507</td></tr><tr><td>OR</td><td>433</td></tr><tr><td>SD</td><td>26</td></tr><tr><td>TN</td><td>140</td></tr><tr><td>SC</td><td>173</td></tr><tr><td>ID</td><td>32</td></tr><tr><td>AZ</td><td>600</td></tr><tr><td>GA</td><td>256</td></tr><tr><td>DC</td><td>21</td></tr><tr><td>IL</td><td>876</td></tr><tr><td>NJ</td><td>1503</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"e2cf644d-96f9-47f7-ad81-780125d3ad4b\"/>\n\n\n## Wrapping Up\n\nRun the following cell to remove the database and all data associated with this lab."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"774cdb52-0076-4e19-97cc-9cf51a81435e","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["DA.cleanup()"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"63963728-99ba-4c08-90e0-3627e25ce13c","inputWidgets":{},"title":""}},"outputs":[{"output_type":"stream","output_type":"stream","name":"stdout","text":["Resetting the learning environment:\n| stopping the stream \"None\"...(0 seconds)\n| dropping the schema \"jtschopp_k017_dbacademy_dewd\"...(3 seconds)\n| removing the working directory \"dbfs:/mnt/dbacademy-users/jtschopp@u.rochester.edu/data-engineering-with-databricks\"...(0 seconds)\n\nValidating the locally installed datasets:\n| listing local files...(8 seconds)\n| completed (8 seconds total)\n\n"]}],"execution_count":0},{"cell_type":"markdown","source":["<i18n value=\"8f3c4c52-b5d9-4f8a-974c-ce5db6430c43\"/>\n\n\nBy completing this lab, you should now feel comfortable:\n* Using PySpark to configure Auto Loader for incremental data ingestion\n* Using Spark SQL to aggregate streaming data\n* Streaming data to a Delta table"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dd5cfbae-5644-4511-a7e2-3841d81ef067","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"DE 6.3L - Using Auto Loader and Structured Streaming with Spark SQL Lab","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2,"mostRecentlyExecutedCommandWithImplicitDF":{"commandId":153415606860911,"dataframes":["_sqldf"]}},"language":"python","widgets":{},"notebookOrigID":153415606860896}},"nbformat":4,"nbformat_minor":0}
